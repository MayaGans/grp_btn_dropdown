# Web Scraping is a computer software technique of extracting information from websites.
# Specifically, we need to transform unstructured data on the web to that which can be formatted for analysis

# here's all the packages we may or may not need
library(data.table)
library(magrittr)
library(httr)
library(jsonlite)
library(stringi)
library(tidyverse)
library(lubridate)
library(RCurl)
library(XML)
library(rjson)
library(emojifont)
library(rlist)
library(Matrix)
library(data.table)
library(gsubfn)
library(textreadr)
library(zoo)
library(rvest)

# the 'get' within the path allows us to exctract specific shows
# adding clauses with and allows for specifics like the month
# and to filter for the band (solo projects are also listed on the website) artistid=1
# I chose month because we can only query 300 shows at a time

# We can automate a script to pull the data per month
# by writing a string containing our query
# then another column for the months we want to paste
# and a third column to concatenate the rest of the query

url <- data.frame(rep(NA, 12))
url$head <- rep("https://api.phish.net/v3/shows/query?apikey=7BA06832CEA80BE2900C&month=", 12)
url$body <- c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12")
url$tail <- rep("&artistid=1", 12)
url$url <- paste(url$head, url$body, url$tail, sep ="")

#head(url)

## Now we can write a loop to extract the data

# create an empty list to store all the data from each month
content <- list()

for (i in 1:nrow(url)) {

  # the GET function allows us to obtain the query
  res <- GET(url=url$url[i])

  # we can parse this content from json to text
  content[[i]] <- content(res, as = 'parsed')

}


showDates <- list()
k = 0

for(j in 1:length(content)) {
  # within the content that is returned for the month point to the
  # location of the data
  for (i in 1:length(content[[j]]$response$data)) {
    # within the data we want to extract for every month j
    # each showdate (i)
    showDates[[i+k]] <- (content[[j]]$response$data[[i]]$showdate)
  }
  # we created a variable k so that when we go to the next month
  # we wont write over our variable
  k <- 1 + length(showDates)
}

# convert to a dataframe
showDates <- as.data.frame(unlist(showDates))
row.names(showDates) <- NULL

# create a dataframe of the showdates
showLoop <- data.frame((showDates))
showLoop$Date <- sort(ymd(showLoop$unlist.showDates.))
showLoop$unlist.showDates. <- NULL

# concatenate the date with the query for that show to extract data
showLoop$Loop <- paste("https://api.phish.net/v3/setlists/get?apikey=365973EBCBD3CC4B6625&showdate=", showLoop$Date, sep="")

# now that we have the date of every show
# we can loop through the query of every setlist to extract any data we like!

#################################################################
#################################################################

# create an empty list to store our results
set <- list()

# for every show
for (i in 1502:nrow(showLoop)) {
  # grab the url
  url <- showLoop$Loop[i]
  # use that url to GET the information from the API
  rest <- GET(url = url)
  # store the content under cont
  cont <- content(rest)

  # some shows don't have data so we want to return those shows as null
  # using an if statements lets deal with the shows that do have data
  if (length(cont$response$data) > 0) {

    # cont$response$data[[1]]$setlistdata
    # contains the setlist data in html which is a hot mess
    set_html <- textreadr::read_html(cont$response$data[[1]]$setlistdata)

    # combine the colon with the previous line
    # because colons are found with "Set 1:", "Set 2:" etc.
    ind.colon <- which(set_html == ":")
    ind.set <- ind.colon - 1

    # paste the colon and the prior line together
    # to concatenate Set 1 and :
    set_html[ind.set] <- paste(set_html[ind.set], set_html[ind.colon], sep = "")

    # make into data frame
    set_html <- data.frame(set_html)
    # call the song column Text
    set_html$Text <- as.character(set_html$set_html)
    set_html$set_html <- NULL

    # remove cells containing brackets, this is a vestage of show notes
    # remove cells only containing special characters
    set_html <- filter(set_html, !grepl("\\[",set_html$Text))
    set_html <- filter(set_html, !grepl("^:$",set_html$Text))

    # create a character string for song names
    # the easiest filter is that they should include all alphabetical characters
    # but then we also need to search for the songs that are numbers
    # call others false
    boo <- c("[a-zA-Z]+", "1999", "555", "5:15")

    # now we can set our songs to true and the rest to false
    set_html$Boolian <- grepl(paste(boo, collapse = "|"), set_html$Text)

    # now we can take the cells that were returned as false
    # and put those into a new column for segues
    ind.FALSE <- which(set_html$Boolian == FALSE)
    ind.CAT <- ind.FALSE - 1

    set_html$Text[ind.CAT] <- paste(set_html$Text[ind.CAT], set_html$Text[ind.FALSE], sep = "__")
    set_html <- filter(set_html, set_html$Boolian == TRUE)
    set_html <- str_split_fixed(set_html$Text, "__", 2)
    colnames(set_html) <- c("Text", "Segue")

    set_html <- data.frame(set_html)

    # now we can remove the rows containing the set
    # and use that data to populate which set each show belongs in
    # we will use a combination of a colon and the colon being
    # the last character in the string (because of RS song 5:15)

    substrRight <- function(x, n){
      substr(x, nchar(x)-n+1, nchar(x))
    }

    set_html$Set <- ifelse(substrRight(as.character(set_html$Text), 1) == ":", paste(set_html$Text), NA)
    set_html$Set <- na.locf(set_html$Set)
    set_html <- filter(set_html, set_html$Text != set_html$Set)

    set[[i]] <- list("Text" = set_html$Text, "Segue" = set_html$Segue, "Set" = set_html$Set)

  } else {
    # in the case that the show had no data set that list element to NA
    set[i] <- NA
  }
  print(i)

}

# name the list elements by the show date
names(set) <- showLoop$Date

##########################################################
##########################################################

# 266 private show
# 407 private show

set[559][[1]]$Text <- c("Magilla", "I Didn't Know", "My Sweet One", "Take the 'A' Train", "Donna Lee",
                        "Unknown Song", "Bill Bailey, Won't You Please Come Home?", "Contact", "Sweet Adeline")
set[559][[1]]$Segue <- rep(",", length(set[559][[1]]$Text))
set[559][[1]]$Set <- c(rep("Set 1:", 5), rep("Set 2:", 4))

# 1311 soundcheck
# 1500 soundcheck

set <- set[-c(266, 407, 1311, 1500)];

# remove sets which don't contain data
set <- set[!is.na(set)]

#########################################################
#########################################################

saveRDS(set, "/Users/mayagans/Documents/PhishData/setlist.RDS")
set <- readRDS("/Users/mayagans/Documents/PhishData/setlist.RDS")


#########################################################
#########################################################

# turn list into dataframe
set.df <- rbindlist(set, fill=TRUE)

# add column names
names <- list()

for(i in 1:length(set)) {
  names[[i]] <- rep(names(set[i]), length(set[[i]]$Text))
}

n <- unlist(names)

set.df$Date <- n

set.df$Date <- lubridate::ymd(set.df$Date)
colnames(set.df) <- c("Title", "Segue", "Set", "Date")

library(rjson)
phish_json <- toJSON(unname(split(set.df, 1:nrow(set.df))))
phish_json <- cat(phish_json)

write.json(phish_json, "phish.txt")

write(phish_json, "phish_setlists.json")


########################################################
########################################################

# create an empty list to store our results
venue <- list()
location <- list()
# for every show
for (i in 1900:nrow(showLoop)) {
  # grab the url
  url <- showLoop$Loop[i]
  # use that url to GET the information from the API
  rest <- GET(url = url)
  # store the content under cont
  cont <- content(rest)

  # some shows don't have data so we want to return those shows as null
  # using an if statements lets deal with the shows that do have data
  if (length(cont$response$data) > 0) {
    venue[i] <- cont$response$data[[1]]$venue
    location[i] <- cont$response$data[[1]]$location
  } else {
    # in the case that the show had no data set that list element to NA
    venue[i] <- NA
    location[i] <- NA
  }
  print(i)

}

venue <- data.frame(venue = unlist(venue), stringsAsFactors = FALSE)
location <- data.frame(location = unlist(location), stringsAsFactors = FALSE)

loc$Date <- showLoop$Date
loc$venue <- ven

showLocation <- data.frame(Date = showLoop$Date, location = location$location, venue = venue$venue) %>%
  na.omit()

t <- data.frame(str_split_fixed(showLocation$location, ",", 2))

showLocation$city <- t$X1
###############
###############

test <- purrr::map(as.character(showLocation$venue), strip_html)
test <- data.frame(unlist(test))

showLocation$venue <- test$unlist.test.
devtools::install_github("dkahle/ggmap", force = TRUE)
test <- as.tibble(showLocation, stringAsFact)
test <- mutate_geocode(test, as.character(showLocation$city))

library(ggmap)


#########################
#########################

# create an empty list to store our results

rating <- list()
# for every show
for (i in 1:nrow(showLoop)) {
  # grab the url
  url <- showLoop$Loop[i]
  # use that url to GET the information from the API
  rest <- GET(url = url)
  # store the content under cont
  cont <- content(rest)

  # some shows don't have data so we want to return those shows as null
  # using an if statements lets deal with the shows that do have data
  if (length(cont$response$data) > 0) {
    rating[i] <- cont$response$data[[1]]$rating
  } else {
    # in the case that the show had no data set that list element to NA
    rating[i] <- NA
  }
  print(i)

}

rating <- data.frame(rating = unlist(rating), stringsAsFactors = FALSE)
rating$Date <- showLoop$Date
rating$weekday <- weekdays(rating$Date)

write.csv(rating, "/Users/mayagans/Desktop/Phish_Ratings.csv")
